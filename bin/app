#!/usr/bin/env raku

#  Copyright (c) 2024. Prediction By Invention https://predictionbyinvention.com/
#
#  THIS SOFTWARE IS PROVIDED "AS IS" WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED,
#  INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A
#  PARTICULAR PURPOSE, AND NON-INFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
#  COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES, OR OTHER LIABILITY, WHETHER
#  IN AN ACTION OF CONTRACT, TORT, OR OTHERWISE, ARISING FROM, OUT OF, OR
#  IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.

use v6.d;
use lib 'lib';
use Util::Logger;
use Cro::HTTP::Server;
use LLM::Messages;
use LLM::Client::OpenAI;
use WWW::ChatUI;

my $APP_LOGGER = Util::Logger.new(namespace => "<TallMountain App>");

my $chat-ui = WWW::ChatUI.new;

# define a supply for the chat requests
my $chat-request-supply = $chat-ui.chat-request-supply;

# define a channel for responses
my $chat-response-channel = $chat-ui.chat-response-channel;

# Start the chat ui server in the background
$chat-ui.start-chat-ui();

# Main application logic
$APP_LOGGER.debug("TallMountain is now running...");
say("TallMountain is now running...");

my $client = LLM::Client::OpenAI.new();

react {

    # Whenever a chat request is received, process it
    whenever $chat-request-supply -> $request {
        $APP_LOGGER.debug("App received user request: '$request'");

        # build a response
        my $messages = LLM::Messages.new;
        $messages.build-messages('You are a helpful assistant.', LLM::Messages.SYSTEM);
        $messages.build-messages($request, LLM::Messages.USER);
        my $response = $client.completion-string($messages.get-messages);
        $APP_LOGGER.debug("App received bot response: '$response'");

        # return the response
        $chat-response-channel.send($response);
    };

    whenever signal(SIGINT) {
        $APP_LOGGER.debug("Shutting down...");
        $chat-response-channel.close;
    }
}